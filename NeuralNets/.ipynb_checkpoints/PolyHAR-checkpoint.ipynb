{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyHAR CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PolyHAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = []\n",
    "y_full = []\n",
    "with open('dataSplit/UCA-data.csv') as f:\n",
    "    next(f)\n",
    "    for l in f:\n",
    "        d = l.split(';')\n",
    "        x_full.append([float(d[1]), float(d[2]), float(d[3])])\n",
    "        y_full.append(1 if 'Positive' in d[4] else 0)\n",
    "\n",
    "x_full = np.array(x_full)\n",
    "y_full = np.array(y_full)\n",
    "\n",
    "#x_full = np.loadtxt('x_test2.csv', delimiter=',')\n",
    "#y_full = np.loadtxt('y_test2.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3 , -0.17,  0.94],\n",
       "       [-0.44,  0.04,  0.98],\n",
       "       [ 0.05, -0.35,  0.46],\n",
       "       ...,\n",
       "       [ 0.06,  1.02,  0.07],\n",
       "       [ 0.04,  1.06,  0.12],\n",
       "       [-0.23,  0.93,  0.26]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 64\n",
    "CLASSES = 2\n",
    "windowscount = np.ceil(x_full.shape[0]/SIZE).astype(int)\n",
    "x_full = np.resize(x_full, (windowscount, SIZE, x_full.shape[-1]))\n",
    "y_full = np.resize(y_full, (windowscount, SIZE, y_full.shape[-1]))\n",
    "y_full = y_full.argmax(axis=-1) # Convert from one-hot to class number to be able to count ocurrences\n",
    "y_full = np.array([np.bincount(w).argmax() for w in y_full]) # Select label with highest number of occurence for each window\n",
    "y_full = to_categorical(y_full, num_classes=CLASSES) # Convert back to one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "RATIO = 0.2 # 20% test, 80% train\n",
    "n = int(len(x_full) * RATIO)\n",
    "\n",
    "# Randomize windows\n",
    "p = np.random.permutation(len(x_full))\n",
    "x_full = x_full[p]\n",
    "y_full = y_full[p]\n",
    "\n",
    "x_test = x_full[-n:]\n",
    "y_test = y_full[-n:]\n",
    "\n",
    "x_train = x_full[:-len(x_test)]\n",
    "y_train = y_full[:-len(y_test)]\n",
    "\n",
    "print(y_test[:,0].shape)\n",
    "#plt.scatter(x_test[:,0,0], y_test[:,0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (0,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15720\\3879174942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'polyhar_x_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'polyhar_y_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'polyhar_x_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'polyhar_y_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (0,newaxis)"
     ]
    }
   ],
   "source": [
    "np.savetxt('polyhar_x_train.csv', x_train.reshape((x_train.shape[0], -1)), delimiter=',', fmt='%s')\n",
    "np.savetxt('polyhar_y_train.csv', y_train, delimiter=',', fmt='%s')\n",
    "np.savetxt('polyhar_x_test.csv', x_test.reshape((x_test.shape[0], -1)), delimiter=',', fmt='%s')\n",
    "np.savetxt('polyhar_y_test.csv', y_test, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 192)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 30)                5790      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 20)                620       \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,452\n",
      "Trainable params: 6,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Input(shape=(SIZE, 3)))\\nmodel.add(Conv1D(filters=10, kernel_size=3, activation='relu'))\\nmodel.add(MaxPool1D(pool_size=6))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(units=6))\\nmodel.add(Dense(units=CLASSES))\\nmodel.add(Activation('softmax')) # SoftMax activation needs to be separate from Dense to remove it later on\\n# EXPLORE Learning Rate\\nopt = tf.keras.optimizers.Adam(lr=11e-3)\\nmodel.summary()\\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\\n\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SIZE, 3)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=30))\n",
    "model.add(Dense(units=20))\n",
    "model.add(Dense(units=CLASSES))\n",
    "model.add(Activation('softmax')) # SoftMax activation needs to be separate from Dense to remove it later on\n",
    "# EXPLORE Learning Rate\n",
    "opt = tf.keras.optimizers.Adam(lr=10e-5)\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SIZE, 3)))\n",
    "model.add(Conv1D(filters=10, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=6))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=6))\n",
    "model.add(Dense(units=CLASSES))\n",
    "model.add(Activation('softmax')) # SoftMax activation needs to be separate from Dense to remove it later on\n",
    "# EXPLORE Learning Rate\n",
    "opt = tf.keras.optimizers.Adam(lr=11e-3)\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6677 - categorical_accuracy: 0.6447 - val_loss: 0.6153 - val_categorical_accuracy: 0.7778\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6027 - categorical_accuracy: 0.6974 - val_loss: 0.5503 - val_categorical_accuracy: 0.7778\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5465 - categorical_accuracy: 0.7105 - val_loss: 0.4919 - val_categorical_accuracy: 0.7778\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4974 - categorical_accuracy: 0.7500 - val_loss: 0.4399 - val_categorical_accuracy: 0.7778\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4484 - categorical_accuracy: 0.8158 - val_loss: 0.3944 - val_categorical_accuracy: 0.7778\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4113 - categorical_accuracy: 0.8816 - val_loss: 0.3544 - val_categorical_accuracy: 0.8333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3750 - categorical_accuracy: 0.8816 - val_loss: 0.3200 - val_categorical_accuracy: 0.8333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3434 - categorical_accuracy: 0.8947 - val_loss: 0.2916 - val_categorical_accuracy: 0.8889\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3181 - categorical_accuracy: 0.9079 - val_loss: 0.2678 - val_categorical_accuracy: 0.9444\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2953 - categorical_accuracy: 0.9079 - val_loss: 0.2471 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2755 - categorical_accuracy: 0.9079 - val_loss: 0.2293 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2588 - categorical_accuracy: 0.9211 - val_loss: 0.2143 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2427 - categorical_accuracy: 0.9211 - val_loss: 0.2011 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2312 - categorical_accuracy: 0.9211 - val_loss: 0.1896 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2207 - categorical_accuracy: 0.9211 - val_loss: 0.1799 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2110 - categorical_accuracy: 0.9342 - val_loss: 0.1717 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2030 - categorical_accuracy: 0.9342 - val_loss: 0.1643 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1956 - categorical_accuracy: 0.9474 - val_loss: 0.1578 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1890 - categorical_accuracy: 0.9474 - val_loss: 0.1520 - val_categorical_accuracy: 0.9444\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1833 - categorical_accuracy: 0.9474 - val_loss: 0.1468 - val_categorical_accuracy: 0.9444\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1783 - categorical_accuracy: 0.9474 - val_loss: 0.1418 - val_categorical_accuracy: 0.9444\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1739 - categorical_accuracy: 0.9474 - val_loss: 0.1373 - val_categorical_accuracy: 0.9444\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1692 - categorical_accuracy: 0.9474 - val_loss: 0.1333 - val_categorical_accuracy: 0.9444\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1654 - categorical_accuracy: 0.9474 - val_loss: 0.1296 - val_categorical_accuracy: 0.9444\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1613 - categorical_accuracy: 0.9474 - val_loss: 0.1265 - val_categorical_accuracy: 0.9444\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1580 - categorical_accuracy: 0.9474 - val_loss: 0.1234 - val_categorical_accuracy: 0.9444\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1548 - categorical_accuracy: 0.9474 - val_loss: 0.1205 - val_categorical_accuracy: 0.9444\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1516 - categorical_accuracy: 0.9605 - val_loss: 0.1176 - val_categorical_accuracy: 0.9444\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1487 - categorical_accuracy: 0.9605 - val_loss: 0.1146 - val_categorical_accuracy: 0.9444\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1458 - categorical_accuracy: 0.9605 - val_loss: 0.1122 - val_categorical_accuracy: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb113a99150>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1122 - categorical_accuracy: 0.9444 - 14ms/epoch - 14ms/step\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb11390d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "tf.Tensor(\n",
      "[[17  1]\n",
      " [ 0  0]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "pred_test = model.predict(x_test)\n",
    "print(tf.math.confusion_matrix(y_test.argmax(axis=1), pred_test.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('polyhar.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(model.input, model.layers[-2].output, name=model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate C for the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: './kerascnn2c_fixed'\n"
     ]
    }
   ],
   "source": [
    "!pip install ./kerascnn2c_fixed\n",
    "# import kerascnn2c\n",
    "# from pathlib import Path\n",
    "# res = kerascnn2c.Converter(output_path=Path('output'),\n",
    "#                            fixed_point=9, # Number of bits for the fractional part\n",
    "#                            number_type='int16_t', # Data type for weights/activations (16 bits quantization)\n",
    "#                            long_number_type='int32_t', # Data type for intermediate results\n",
    "#                            number_min=-(2**15), # Minimum value for the data type\n",
    "#                            number_max=(2**15)-1 # Maximum value for the data type\n",
    "#                           ).convert_model(model)\n",
    "# with open('polyhar_model2.h', 'w') as f:\n",
    "#     f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
